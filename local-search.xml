<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>基于YOLOV5的裂纹检测</title>
    <link href="/2022/10/22/YOLOV5/"/>
    <url>/2022/10/22/YOLOV5/</url>
    
    <content type="html"><![CDATA[<h2 id="项目实战：基于YOLO系列的道路裂纹识别"><a href="#项目实战：基于YOLO系列的道路裂纹识别" class="headerlink" title="项目实战：基于YOLO系列的道路裂纹识别"></a>项目实战：基于YOLO系列的道路裂纹识别</h2><p>导师临时给的一个小项目，任务是对道路的裂纹进行检测。由于时间比较紧，没有去跑一些论文的代码，先把结果跑出来再说。</p><h3 id="数据集概览"><a href="#数据集概览" class="headerlink" title="数据集概览"></a>数据集概览</h3><p>首先我们先看一下数据长什么样子：<br>DATA<br>——Annotations<br>——images<br>一个存放的是图片，一个存放的是人工打的标签<br>数据集大概张这个样子：<br><img src="https://img-blog.csdnimg.cn/811dc0d7fd374e788a429c1426c0b644.png"><br>类别有两类，PIT和Crack，裂纹和坑洼。<br>标签的分布的话，是这样的<br><img src="https://img-blog.csdnimg.cn/4bf3d7a526f54bb29bcec87071643805.png" alt="在这里插入图片描述"><br>可以看出，pit的标签数量是远小于crack标签的数量的，但是yolov5里面有操作来对抗这样的情况，所以暂且不用担心。</p><h3 id="基于自己数据集训练YOLOV5"><a href="#基于自己数据集训练YOLOV5" class="headerlink" title="基于自己数据集训练YOLOV5"></a>基于自己数据集训练YOLOV5</h3><p>在此，作者参考了一篇blog，在此表示感谢！<br><a href="http://t.csdn.cn/IqTiW">http://t.csdn.cn/IqTiW</a></p><h4 id="配置环境"><a href="#配置环境" class="headerlink" title="配置环境"></a>配置环境</h4><p>不得不说DL最麻烦的怕就是配置环境了，本文使用的环境是：<br>CUDA11.5<br>Pytorch 1.11.0<br>torchvision 0.12.0<br>yolov5 v6.0<br>Python 3.9<br>pytorch的配置方法就不说了，由于服务器是隔绝网络的，因此需要先把whl文件下载下来，传到上面再安装。注意torchvision的版本和pytorch要一致，不然跑代码时会报错<br>其余环境可以按照yolov5-master里的requirements.txt的要求来安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install -r requirements.txt -i  https://pypi.tuna.tsinghua.edu.cn/simple<br></code></pre></td></tr></table></figure><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><p>需要对数据集划分成训练集和验证集，这里直接用vim命令新建一个python文件，命名为split_train_val.py，里面内容是</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># coding:utf-8</span><br><br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">import</span> random<br><span class="hljs-keyword">import</span> argparse<br><br>parser = argparse.ArgumentParser()<br><span class="hljs-comment">#xml文件的地址，根据自己的数据进行修改 xml一般存放在Annotations下</span><br>parser.add_argument(<span class="hljs-string">&#x27;--xml_path&#x27;</span>, default=<span class="hljs-string">&#x27;Annotations&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;input xml label path&#x27;</span>)<br><span class="hljs-comment">#数据集的划分，地址选择自己数据下的ImageSets/Main</span><br>parser.add_argument(<span class="hljs-string">&#x27;--txt_path&#x27;</span>, default=<span class="hljs-string">&#x27;ImageSets/Main&#x27;</span>, <span class="hljs-built_in">type</span>=<span class="hljs-built_in">str</span>, <span class="hljs-built_in">help</span>=<span class="hljs-string">&#x27;output txt label path&#x27;</span>)<br>opt = parser.parse_args()<br><br>trainval_percent = <span class="hljs-number">1.0</span>  <span class="hljs-comment"># 训练集和验证集所占比例。 这里没有划分测试集</span><br>train_percent = <span class="hljs-number">0.9</span>     <span class="hljs-comment"># 训练集所占比例，可自己进行调整</span><br>xmlfilepath = opt.xml_path<br>txtsavepath = opt.txt_path<br>total_xml = os.listdir(xmlfilepath)<br><span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(txtsavepath):<br>    os.makedirs(txtsavepath)<br><br>num = <span class="hljs-built_in">len</span>(total_xml)<br>list_index = <span class="hljs-built_in">range</span>(num)<br>tv = <span class="hljs-built_in">int</span>(num * trainval_percent)<br>tr = <span class="hljs-built_in">int</span>(tv * train_percent)<br>trainval = random.sample(list_index, tv)<br>train = random.sample(trainval, tr)<br><br>file_trainval = <span class="hljs-built_in">open</span>(txtsavepath + <span class="hljs-string">&#x27;/trainval.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>file_test = <span class="hljs-built_in">open</span>(txtsavepath + <span class="hljs-string">&#x27;/test.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>file_train = <span class="hljs-built_in">open</span>(txtsavepath + <span class="hljs-string">&#x27;/train.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br>file_val = <span class="hljs-built_in">open</span>(txtsavepath + <span class="hljs-string">&#x27;/val.txt&#x27;</span>, <span class="hljs-string">&#x27;w&#x27;</span>)<br><br><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> list_index:<br>    name = total_xml[i][:-<span class="hljs-number">4</span>] + <span class="hljs-string">&#x27;\n&#x27;</span><br>    <span class="hljs-keyword">if</span> i <span class="hljs-keyword">in</span> trainval:<br>        file_trainval.write(name)<br>        <span class="hljs-keyword">if</span> i <span class="hljs-keyword">in</span> train:<br>            file_train.write(name)<br>        <span class="hljs-keyword">else</span>:<br>            file_val.write(name)<br>    <span class="hljs-keyword">else</span>:<br>        file_test.write(name)<br><br>file_trainval.close()<br>file_train.close()<br>file_val.close()<br>file_test.close()<br><br></code></pre></td></tr></table></figure><p>会生成对应的train,val的文件<br>同时，要将xml格式转为txt格式，再写一个程序，命名为xml_to_yolo.py</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># -*- coding: utf-8 -*-</span><br><span class="hljs-keyword">import</span> xml.etree.ElementTree <span class="hljs-keyword">as</span> ET<br><span class="hljs-keyword">import</span> os<br><span class="hljs-keyword">from</span> os <span class="hljs-keyword">import</span> getcwd<br><br>sets = [<span class="hljs-string">&#x27;train&#x27;</span>, <span class="hljs-string">&#x27;val&#x27;</span>, <span class="hljs-string">&#x27;test&#x27;</span>]<br>classes = [<span class="hljs-string">&quot;light&quot;</span>, <span class="hljs-string">&quot;post&quot;</span>]   <span class="hljs-comment"># 改成自己的类别</span><br>abs_path = os.getcwd()<br><span class="hljs-built_in">print</span>(abs_path)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert</span>(<span class="hljs-params">size, box</span>):<br>    dw = <span class="hljs-number">1.</span> / (size[<span class="hljs-number">0</span>])<br>    dh = <span class="hljs-number">1.</span> / (size[<span class="hljs-number">1</span>])<br>    x = (box[<span class="hljs-number">0</span>] + box[<span class="hljs-number">1</span>]) / <span class="hljs-number">2.0</span> - <span class="hljs-number">1</span><br>    y = (box[<span class="hljs-number">2</span>] + box[<span class="hljs-number">3</span>]) / <span class="hljs-number">2.0</span> - <span class="hljs-number">1</span><br>    w = box[<span class="hljs-number">1</span>] - box[<span class="hljs-number">0</span>]<br>    h = box[<span class="hljs-number">3</span>] - box[<span class="hljs-number">2</span>]<br>    x = x * dw<br>    w = w * dw<br>    y = y * dh<br>    h = h * dh<br>    <span class="hljs-keyword">return</span> x, y, w, h<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">convert_annotation</span>(<span class="hljs-params">image_id</span>):<br>    in_file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;D:/Yolov5/yolov5/VOCData/Annotations/%s.xml&#x27;</span> % (image_id), encoding=<span class="hljs-string">&#x27;UTF-8&#x27;</span>)<br>    out_file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;D:/Yolov5/yolov5/VOCData/labels/%s.txt&#x27;</span> % (image_id), <span class="hljs-string">&#x27;w&#x27;</span>)<br>    tree = ET.parse(in_file)<br>    root = tree.getroot()<br>    size = root.find(<span class="hljs-string">&#x27;size&#x27;</span>)<br>    w = <span class="hljs-built_in">int</span>(size.find(<span class="hljs-string">&#x27;width&#x27;</span>).text)<br>    h = <span class="hljs-built_in">int</span>(size.find(<span class="hljs-string">&#x27;height&#x27;</span>).text)<br>    <span class="hljs-keyword">for</span> obj <span class="hljs-keyword">in</span> root.<span class="hljs-built_in">iter</span>(<span class="hljs-string">&#x27;object&#x27;</span>):<br>        difficult = obj.find(<span class="hljs-string">&#x27;difficult&#x27;</span>).text<br>        <span class="hljs-comment">#difficult = obj.find(&#x27;Difficult&#x27;).text</span><br>        cls = obj.find(<span class="hljs-string">&#x27;name&#x27;</span>).text<br>        <span class="hljs-keyword">if</span> cls <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> classes <span class="hljs-keyword">or</span> <span class="hljs-built_in">int</span>(difficult) == <span class="hljs-number">1</span>:<br>            <span class="hljs-keyword">continue</span><br>        cls_id = classes.index(cls)<br>        xmlbox = obj.find(<span class="hljs-string">&#x27;bndbox&#x27;</span>)<br>        b = (<span class="hljs-built_in">float</span>(xmlbox.find(<span class="hljs-string">&#x27;xmin&#x27;</span>).text), <span class="hljs-built_in">float</span>(xmlbox.find(<span class="hljs-string">&#x27;xmax&#x27;</span>).text), <span class="hljs-built_in">float</span>(xmlbox.find(<span class="hljs-string">&#x27;ymin&#x27;</span>).text),<br>             <span class="hljs-built_in">float</span>(xmlbox.find(<span class="hljs-string">&#x27;ymax&#x27;</span>).text))<br>        b1, b2, b3, b4 = b<br>        <span class="hljs-comment"># 标注越界修正</span><br>        <span class="hljs-keyword">if</span> b2 &gt; w:<br>            b2 = w<br>        <span class="hljs-keyword">if</span> b4 &gt; h:<br>            b4 = h<br>        b = (b1, b2, b3, b4)<br>        bb = convert((w, h), b)<br>        out_file.write(<span class="hljs-built_in">str</span>(cls_id) + <span class="hljs-string">&quot; &quot;</span> + <span class="hljs-string">&quot; &quot;</span>.join([<span class="hljs-built_in">str</span>(a) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> bb]) + <span class="hljs-string">&#x27;\n&#x27;</span>)<br><br>wd = getcwd()<br><span class="hljs-keyword">for</span> image_set <span class="hljs-keyword">in</span> sets:<br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;D:/Yolov5/yolov5/VOCData/labels/&#x27;</span>):<br>        os.makedirs(<span class="hljs-string">&#x27;D:/Yolov5/yolov5/VOCData/labels/&#x27;</span>)<br>    image_ids = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;D:/Yolov5/yolov5/VOCData/ImageSets/Main/%s.txt&#x27;</span> % (image_set)).read().strip().split()<br>   <br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(<span class="hljs-string">&#x27;D:/Yolov5/yolov5/VOCData/dataSet_path/&#x27;</span>):<br>        os.makedirs(<span class="hljs-string">&#x27;D:/Yolov5/yolov5/VOCData/dataSet_path/&#x27;</span>)<br>     <br>    list_file = <span class="hljs-built_in">open</span>(<span class="hljs-string">&#x27;dataSet_path/%s.txt&#x27;</span> % (image_set), <span class="hljs-string">&#x27;w&#x27;</span>)<br>    <span class="hljs-comment"># 这行路径不需更改，这是相对路径</span><br>    <span class="hljs-keyword">for</span> image_id <span class="hljs-keyword">in</span> image_ids:<br>        list_file.write(<span class="hljs-string">&#x27;D:/Yolov5/yolov5/VOCData/images/%s.jpg\n&#x27;</span> % (image_id))<br>        convert_annotation(image_id)<br>    list_file.close()<br><br></code></pre></td></tr></table></figure><p>注意要将路径修改为自己的路径<br>运行后会生成labels和dataSet_path文件夹，其中<br>labels会保存么给图片的标注的class x_center y_center width height</p><h4 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h4><p>配置myvoc.yaml文件，模板如下</p><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs yaml"><span class="hljs-attr">train:</span> <span class="hljs-string">D:/Yolov5/yolov5/VOCData/dataSet_path/train.txt</span><br><span class="hljs-attr">val:</span> <span class="hljs-string">D:/Yolov5/yolov5/VOCData/dataSet_path/val.txt</span><br><br><span class="hljs-comment"># number of classes</span><br><span class="hljs-attr">nc:</span> <span class="hljs-number">2</span><br><br><span class="hljs-comment"># class names</span><br><span class="hljs-attr">names:</span> [<span class="hljs-string">&quot;pit&quot;</span>, <span class="hljs-string">&quot;crack&quot;</span>]<br><br></code></pre></td></tr></table></figure><p>同时修改模型的配置文件，<br><img src="https://img-blog.csdnimg.cn/2881f7f4a162423c8c35b3bab33a81d0.png" alt="在这里插入图片描述"><br>只需要修改yaml里面的类别数量即可<br>修改train.py里面的超参数<br><img src="https://img-blog.csdnimg.cn/cb0325a5d1bf4a4f8cf4d7e39f9afc4d.png" alt="在这里插入图片描述"><br>然后就可以开始训练啦<br>训练过程中如果想看损失曲线啥的，可以用tensorboard可视化</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">tensorboard --logdir=runs<br></code></pre></td></tr></table></figure><h3 id="训练结果"><a href="#训练结果" class="headerlink" title="训练结果"></a>训练结果</h3><p>P-R曲线长这个样子，<br><img src="https://img-blog.csdnimg.cn/aca0917552914182b493bd5d6436c528.png"><br>验证集上效果如下：<br><img src="https://img-blog.csdnimg.cn/10163c7c1e3f4565a1438efcdbbe224c.png"><br>可以发现对于pit的识别效果太差，后面将会做进一步改进</p>]]></content>
    
    
    
    <tags>
      
      <tag>YOLO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>【论文精读】Speech Anti-spoofing</title>
    <link href="/2022/09/29/%E3%80%90%E6%97%A0%E6%A0%87%E9%A2%98%E3%80%91%20(3)/"/>
    <url>/2022/09/29/%E3%80%90%E6%97%A0%E6%A0%87%E9%A2%98%E3%80%91%20(3)/</url>
    
    <content type="html"><![CDATA[<p>今天是第二天啦，昨天写完笔记忘记上传了233今天就一起上传咯。<br>继续阅读论文，今天是Background部分和Experiment部分。来吧！</p><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h2><p>本章重点在于总结前人的工作</p><h3 id="2-1-神经网络"><a href="#2-1-神经网络" class="headerlink" title="2.1 神经网络"></a>2.1 神经网络</h3><p>是一些对于神经网络发展的概述，在此不赘述</p><h3 id="2-2-主要神经网络框架"><a href="#2-2-主要神经网络框架" class="headerlink" title="2.2 主要神经网络框架"></a>2.2 主要神经网络框架</h3><p>2.1.1 卷积神经网络<br>卷积神经网络的核心在于卷积核，即一个滤波器（小框）在图像上不断扫描。卷积神经网络的重要论文之一是AlexNet,能够在ILSVRC竞赛上达到15.3%的错误率。但是，CNN的主要问题在于这个网络是如何学习的、如何预测的。为了探究，MIT的研究者们发明了一种方法，建立类别激活图像（CAM），来看看图像中的那一部分与最终的分类结果最为相关。<br>在前任工作的基础上， Matthew D. Zeiler and Rob Fergus等人发表了一篇论文，认为需要对实验方法和结果进行分析，才能更好地改进实验。他们使用了一种网络叫做反卷积网络，能够对卷积网络中的各层进行可视化，发现哪一个特征被提取了出来，帮助他们了解那一部分与问题相关。<br>2.2.2 Long-Short-Term-Memory Networks（长短期记忆网络）<br>卷积神经网络有两个问题：</p><ol><li>无法处理时序数列</li><li>输入数据的格式大小必须固定</li></ol><p>LSTM网络是基于RNN（循环神经网络）搭建起来的，每一层的输出可以反过来成为该层的输入，这样能够使网络带有一定的记忆功能，也就可以处理序列数据了。<br>同时，有作者提出使用两个LSTM网络来解决输入长度限制的问题，一个LSTM网络将输入转化为特定长度的向量，另外一个网络则把这个向量解码转化为输出。<br>但是这也会带来一个问题，网络的性能随着输入长度的增加而下降。为了解决这个问题，研究这么提出了对齐机制。对齐机制与编码-解码器的最大区别就在于，后者注重将整个输出变为向量，而前者更注重与去寻找哪个输入部分对输出影响最大。作者根据这个思想提出了双向递归神经网络，未来输入对网络也会有影响，将正常顺序和混乱顺序的词语都当做输入进行训练，对网络的性能能够较好的提升（未来有可能的输入）<br>2.2.3 生成对抗网络（GANs）<br>在此不赘述</p><h3 id="2-3-数字音频与机器学习"><a href="#2-3-数字音频与机器学习" class="headerlink" title="2.3 数字音频与机器学习"></a>2.3 数字音频与机器学习</h3><p>硬件的提升使神经网络有更复杂的模型，能处理多格式的数据，比如数字音频处理。<br>2.3.1 音频合成<br>2016年，Google的研究者们发表了一篇名为《Wavenet: A generative model for raw audio》<br>的论文，使用了CNN而不是RNN来合成语音（上文已经提过为什么不用CNN来处理音频）作者在CNN上使用了“dilated causal cnovolutional layers”（扩张的因果卷积层），<br>通过跳过输出的几步来扩大感受野，这样高层的输入就依赖于低层网络的输出。结果显示WaveNet的性能比RNN要好，而且与人的自然语言也更加契合。但是也有缺点：输出时间过长，需要90Min来生成1s的语音。DeepVoice3改进了这个网络，使其能够在更短时间内生成语音。<br>2.3.2 音频处理<br>2.3.3 Speech Processing</p><h3 id="2-4-语音合成"><a href="#2-4-语音合成" class="headerlink" title="2.4 语音合成"></a>2.4 语音合成</h3><p>想要对合成语音进行检测，就要明白语音是怎么合成的<br>2.4.1 传统方法<br>语音合成有两种方式：</p><ul><li>硬件方面：声道复制</li><li>软件方面：基于文本来合成语音或者将一种语音转化为另一种语音<br>可以通过语音拼接、调频参数或者生物发生力学来合成语音</li></ul><p> 通过以上分析，我们发现要生成一个好的合成语音检测器，需要：</p><ul><li>相当大的数据集</li><li>数据质量要高，涵盖最新技术产生的语音</li><li>人们对于合成语音的表现，反应<h2 id="3-DataSets"><a href="#3-DataSets" class="headerlink" title="3 DataSets"></a>3 DataSets</h2>在此不赘述作者使用的数据集<h2 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4 Experiment"></a>4 Experiment</h2><h3 id="4-1-Tools"><a href="#4-1-Tools" class="headerlink" title="4.1 Tools"></a>4.1 Tools</h3>音频处理使用的是SoX工具，数据挖掘分析使用的是Weka工具，深度学习使用的是Tensorflow框架和Keras工具<h3 id="4-2-人类分辨合成语音"><a href="#4-2-人类分辨合成语音" class="headerlink" title="4.2 人类分辨合成语音"></a>4.2 人类分辨合成语音</h3>作者开展了一份调查，对人们能否分辨合成语音。结果显示，人们会错掉3个合成语音中的1个。但若果是高性能算法，人们有一半时间是错的。<br><img src="https://img-blog.csdnimg.cn/47d494ef6bfb4560ad7d5825dd031254.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_17,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><h3 id="4-3-合成语音检测"><a href="#4-3-合成语音检测" class="headerlink" title="4.3 合成语音检测"></a>4.3 合成语音检测</h3></li></ul><p>4.3.1 传统频率分析<br>首先要从每个音频中提取一个表示（比如STFT矩阵），平均这个特征得到一个频率激活向量，将这个向量和标签当作输入到WEKA来和主要的算法进行对比。方法包括：</p><ul><li><strong>快速傅里叶变换</strong>：</li><li><strong>短时傅里叶变换</strong>：</li><li><strong>梅尔频谱图</strong>：</li><li><strong>梅尔频率系数</strong></li><li><strong>恒Q变换</strong></li></ul><p>结果如下：<br><img src="https://img-blog.csdnimg.cn/89912768c8a2490094c26782ccf43977.png#pic_center" alt="在这里插入图片描述"><br>从上图可以发现，用MFCC提取的特征辅以随机森林方法能够将验证准确率达到98.54%，这说明尽管频率分析的方法可能不是最好的方法，但使用频率的信息能够帮助我们达到高的准确率<br>为了更好地探究分类准确率和频谱图之间的差异，我们决定调研哪个频率范围对于分类任务来说更重要，在本文中，我们使用了Chi-square和Information Gain来生成频率分类激活图（Frequency classification activation map）<br><img src="https://img-blog.csdnimg.cn/7661a2a7c51d4414bed6c31fc1a7aa74.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_17,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>在上图中，红色意味着对分类的重要性高，绿色意味着重要性低。通过上图可以发现，高频段对于分辨合成音频更加重要。<br>4.3.2 深度学习<br>我们将频率特征用频谱图表示，这样就可以用图像分类的神经网络进行训练。<br><img src="https://img-blog.csdnimg.cn/63212ad48dfb4782840944671921af05.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>训练结果如下：<br><img src="https://img-blog.csdnimg.cn/34062d85681c4c05876a150ad2cbfe61.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>上文已经提到，利用CAM图来帮助我们获取那个部分对输出影响最大，以狗的分类为例，下图说明狗的眼镜和耳朵对分类影响结果最大：<br><img src="https://img-blog.csdnimg.cn/bdef96ed886a43db8039aaf1a238b080.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_14,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>相似的，我们可以将方法运用到频谱的分类上。如果我们随机从真实语音和合成语音中选取几个数据，对他们做CAM图，结果如下：<br><img src="https://img-blog.csdnimg.cn/8d9db3c029d44b9a9966ec45c03edd8d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_18,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>如果我们2将所有数据的频谱图做平均，再做CAM图，结果如下：<br><img src="https://img-blog.csdnimg.cn/b19d0defdf4b44e897b49a4e7a24043f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_17,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>从上面的图像可以看出，高频段对于识别真实语音很重要。这可能与真实语音中常常含有环境噪声有关。后续关于环境噪声和波形对分类的影响详见附录（明天再看）<br>在现有数据集上，我们发现传统方法和深度学习方法表现得都很好，下一章我们将尝试一种新的TTS算法，再来测试一下两种方法优劣。</p><h3 id="4-4-新的合成语音算法"><a href="#4-4-新的合成语音算法" class="headerlink" title="4.4 新的合成语音算法"></a>4.4 新的合成语音算法</h3><p>我们不仅要看模型在训练数据集上的性能，更关注其泛化能力。因此，我们采用了一种船新的算法（Google TTS Wavenet）来测试两类模型的泛化能力<br>4.4.1 传统方法<br>结果如下：<br><img src="https://img-blog.csdnimg.cn/da55d7bf60ea49968c2a6b00954d01c4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_15,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>我们可以发现，在训练集里表现很好的MFCC音频表示方法在实验中准确率下降了。然而CQT表征在两个数据集里表现都很好<br>对于FCAM图来看，发现和第一次实验一样，即便是新的算法，合成语音与真实语音的最大差别还是在高频段<br>4.4.2 深度学习<br>结果如下：<br><img src="https://img-blog.csdnimg.cn/c8c37236ba964497857b4d400930727d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_15,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>结合之前实验和经典算法，我们也许能够认为CQT的性能最好</p>]]></content>
    
    
    
    <tags>
      
      <tag>PaperReading</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Batch Normalization</title>
    <link href="/2022/09/29/%E3%80%90%E6%97%A0%E6%A0%87%E9%A2%98%E3%80%91%20(2)/"/>
    <url>/2022/09/29/%E3%80%90%E6%97%A0%E6%A0%87%E9%A2%98%E3%80%91%20(2)/</url>
    
    <content type="html"><![CDATA[<h2 id="1-Batch-Normalization解决的问题"><a href="#1-Batch-Normalization解决的问题" class="headerlink" title="1 Batch Normalization解决的问题"></a>1 Batch Normalization解决的问题</h2><p>在训练模型时，可能会比较难收敛，或者收敛很慢。以下图为例，当一个线性模型中，$x_1$与$x_2$的尺度不一样时，$w_1$与$w_2$的范围也就不一样。<br><img src="https://img-blog.csdnimg.cn/0f3bd5af44dd49048e6c1e10df508d82.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>为了让所有的输入有相同的维度，很容易想到归一化(feature normalization)<br><img src="https://img-blog.csdnimg.cn/13d04200ce8d4c0da2ee1b39ba36ec4f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>问题又来了，如果对于$x_1$归一化后，产生的$a_1$对$W^2$层也会有很大的差异怎么办？那我们就需要在sigmoid函数之前或之后做normalization。（实际之前之后影响不大）<br><img src="https://img-blog.csdnimg.cn/a61f97542edc4def94113ca155b0b193.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><strong>注：</strong> 在进行normalization之后，整个神经网络变得更大了。因为如果现在$z^1$改变$\Delta$，不像之前只改变$a^1$，而是会通过改变$\mu$，$\sigma$来影响$a^1,a^2,a^3$。因此做反向传播时要注意！</p><h2 id="2-Batch-Normalization的过程"><a href="#2-Batch-Normalization的过程" class="headerlink" title="2 Batch Normalization的过程"></a>2 Batch Normalization的过程</h2><p><img src="https://img-blog.csdnimg.cn/60f4bb17e34f4af7ad0c44d9d2816ef4.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>从原始数据集上取小批量batch，然后对这个batch做normalization，再做一个分布变换。<br>BN在训练过程中比较容易实施，但在testing时，我们由于数据量可能不够一个batch，因此也就无法计算$\mu,\sigma$。所以，在测试中，我们就用训练时产生的$\mu,\sigma$来代替。</p><h2 id="3-Batch-Normalization为什么有用？"><a href="#3-Batch-Normalization为什么有用？" class="headerlink" title="3 Batch Normalization为什么有用？"></a>3 Batch Normalization为什么有用？</h2><p>在原文中，作者提出了一个问题：<strong>interval covariate shift</strong>（内在协方差偏移）<br>ICS问题说的是，在一个神经网络中，由于参数频繁在更新，距离输出层越近的层输入变化会非常剧烈。因此对于每一层输入的数据不再独立同分布<br><img src="https://img-blog.csdnimg.cn/f17c47b8a35642cfad815bd934451430.png#pic_center" alt="在这里插入图片描述"><br>而输入数据不同分布会导致下列问题：</p><ul><li>违反独立同分布假设：假设训练数据和测试数据都满足独立同分布，否则训练效果就会很差。<br>以下图为例，在左边的模型中，我们学习了一个识别黑猫的模型，但右边又输入了其他猫的图片，不适用于本模型，会导致训练难度加大或重新训练，难以收敛。<br><img src="https://img-blog.csdnimg.cn/9c8de96582bd4aca96ea50e19c3f0c36.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_8,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></li><li>梯度消失<br>由于参数变化比较大，输出不太稳定，如果模型的输入在过大或过小的位置，会导致BP时出现梯度消失的情况<br><img src="https://img-blog.csdnimg.cn/639ea2f56acf41b3b4e8156b22512b1c.png#pic_center" alt="在这里插入图片描述"></li><li>上层网络要不断适应参数的变化，导致学习速度太慢<br>而提出BN的作者认为，BN能够减少网络中的ICS问题。然而，在一片名为《How Does Batch Normalization Help Optimization?》的论文中，作者并不这么认为。下面我们简略讲一下该论文的工作和思想。<br>首先，作者对比了没有使用BN和使用BN后的神经网络的训练情况，发现加了BN后的网络确实训练的更快了。<br><img src="https://img-blog.csdnimg.cn/898995d87ff94ff699ae7a167f5604e5.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_14,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>接着，作者提出了两个问题：</li><li>BN的有效性真的与ICS问题有关吗？</li><li>BN稳定了层输入分布真的减少了ICS吗？<br>针对第一个问题。作者在使用了BN后的数据上加入了一个随机噪声，使输入的分布仍存在较大差异。但结果发现，训练的performance与没加入噪声的BN没有很大差别。<br><img src="https://img-blog.csdnimg.cn/592807422c204c49a2c90832650048ac.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>针对问题二，作者计算了对每层网络输入输出的梯度差和cos值，发现对于DLN网络来说，BN反倒使前后梯度差增加了，对应的ICS问题更严重了。</li></ul><p><img src="https://img-blog.csdnimg.cn/392d85c73a804cbca938478b2e496a45.png#pic_center" alt="在这里插入图片描述"><br>接着，作者提出了自己的观点：BN使优化平面更加平滑，作者计算了$\beta-smoothness$的值以及L-Lipschitz常数来代表loss的光滑程度，发现BN能使平面更加平滑，也就更加容易训练了<br><img src="https://img-blog.csdnimg.cn/158667f8546648a080d9b1984aa65bfb.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/b72e3a22c90849ff917874a3ea06dc5d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_19,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>以上内容来自于：李宏毅2021机器学习<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=15&amp;share_source=copy_web">https://www.bilibili.com/video/BV1Wv411h7kN?p=15&amp;share_source=copy_web</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Neural Networks</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAN(二)</title>
    <link href="/2022/09/29/%E3%80%90%E6%97%A0%E6%A0%87%E9%A2%98%E3%80%91%20(1)/"/>
    <url>/2022/09/29/%E3%80%90%E6%97%A0%E6%A0%87%E9%A2%98%E3%80%91%20(1)/</url>
    
    <content type="html"><![CDATA[<p>GAN生成对抗网络<br><img src="https://img-blog.csdnimg.cn/da4ead9e346b42c8980c0c6d83c9ac44.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_14,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>输入分布，输出分布<br>为什么是分布？</p><ol><li>任务需要创造力：同样输入可能有多种输出<br>比如创造对话、创造图像等等。即使是同一个输出，比如红眼睛，也有可能产生不同的图像。<br><img src="https://img-blog.csdnimg.cn/3c09dfb75c434fe081a51b788d1b6564.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/a1c54fcd2f15492d99ced0e37404227e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>新概念：Discriminator(nerual network)<br>输入图片，输出数字</li></ol><p><img src="https://img-blog.csdnimg.cn/3ba63fc7646945a78defd9782f0d0963.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>Basic idea of GAN<br><img src="https://img-blog.csdnimg.cn/12517a7a9043449fbd7f262663d9bec7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><strong>演化</strong><br>Generator 和Discriminator的对抗。<br>Generator和Discriminator共同进化。</p><p><img src="https://img-blog.csdnimg.cn/9e7b6ee3f28443e2a18be74b1f9bb657.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><strong>算法：</strong><br>初始化参数<br>step1:固定generator，更新discriminator</p><p><img src="https://img-blog.csdnimg.cn/ebf546c20f0d4e8386bbff6e38ee2b8d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_12,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>训练目标：分辨真正图片和生成图片之间的差异（分类或回归）<br><img src="https://img-blog.csdnimg.cn/d0d3d9955c44411db1907a6bc9182870.png#pic_center" alt="在这里插入图片描述"><br>step2:固定discriminator，训练generator，来以假乱真<br><img src="https://img-blog.csdnimg.cn/bec2b846f81c482caefde0679e9ac693.png#pic_center" alt="在这里插入图片描述"><br>如果generator产生的图片，discriminator给的分高，说明和真实图片更像。也就是说，生成器和辨别器可以拼接成一个大的神经网络，输出一个分数。而且中间有一层非常长（对应generator产生的图片）但是要注意，此时是不更新discriminator那几层的参数的！<br><img src="https://img-blog.csdnimg.cn/ea3e21d753c947dcb8f36588a6faf18b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>step3:反复训练，迭代<br>GAN的用处：</p><ol><li>产生动漫人物：styleGAN</li><li>产生人脸:Progressive GAN<br>连续做内插：<img src="https://img-blog.csdnimg.cn/f1f1ae5c8d2945729e7e4630e9d016e7.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_9,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>渐变！<br>Theory behind GAN<br>训练目标：<br><img src="https://img-blog.csdnimg.cn/fa8b782a261e4d0eb2a8a23ddfe7f61a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>生成器产生的分布与真实分布越接近越好</li></ol><p><img src="https://img-blog.csdnimg.cn/c4060a1c33af48f29eea8705e4cc020b.png#pic_center" alt="在这里插入图片描述"><br>问题来了：怎么衡量divergence?<br>计算很复杂，而且我们也不知道PG和PDATA真正的分布长什么样子。因此我们用sample来计算divergence<br><img src="https://img-blog.csdnimg.cn/19d8834e8bd043d6844417850edc173c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/2e3213c286a648afbd5db54cb64f7159.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>Binary Classification：交叉熵<br><img src="https://img-blog.csdnimg.cn/b279f220afc549c2ab215ddd5afad4e1.png#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/fb82316c4c96409581220b7225297814.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>GAN不好训练<br>JS Divergence特性：<br>Pg与Pdata重叠部分很小：<br>1高维空间的低维东西<br>2 Sample不够多，来表示实际的分布<br>两个分布没有重叠，那么JS divergence永远是log2<br><img src="https://img-blog.csdnimg.cn/3cd50cb650304c729497b141bca217d6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/db1dd9a5b66b4c54ab0195147b655bbc.png#pic_center" alt="在这里插入图片描述"><br>那有没有好的divergence<br><img src="https://img-blog.csdnimg.cn/aa33e95a0d61457eafef53ecc5ec08ed.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/dfa836eade394afba3e1df1e3fe9ddae.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>穷举，找到最小的路径</p><p>训练技巧：<br>5. WGAN<br><img src="https://img-blog.csdnimg.cn/72a28d8128154fac9d684c127ab2f83f.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br><img src="https://img-blog.csdnimg.cn/8565b17752ee4bf7aef0c5a467fafc84.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>Spectral Normalization</p>]]></content>
    
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GAN(一)</title>
    <link href="/2022/09/29/546/"/>
    <url>/2022/09/29/546/</url>
    
    <content type="html"><![CDATA[<h2 id="一、为什么需要GAN"><a href="#一、为什么需要GAN" class="headerlink" title="一、为什么需要GAN"></a>一、为什么需要GAN</h2><p>首先明确，GAN仍是神经网络。而在现实中，我们相同的输入可能需要不同的输出（对应一个分布），这就需要GAN了。<br>比如，chatbox或者绘制动漫图像中，我们输入为红眼特征，输出的图像也不一样。<br><img src="https://img-blog.csdnimg.cn/3c09dfb75c434fe081a51b788d1b6564.png#pic_center" alt="在这里插入图片描述"></p><h2 id="二、GAN思想"><a href="#二、GAN思想" class="headerlink" title="二、GAN思想"></a>二、GAN思想</h2><p>GAN是基于<strong>演化</strong>思想提出的神经网络（或者进化）。在GAN网络中，进化的是Generator和Discriminator。<br><strong>Generator</strong>:生成器<br><img src="https://img-blog.csdnimg.cn/a1c54fcd2f15492d99ced0e37404227e.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>输入向量，输出向量均服从特定分布。<br><strong>Discriminator（辨别器）</strong>：输入图片（实际为向量），输出数字，代表图片与实际图片的相似度，或者真实度，真实度越高，数值越大。<br>演化就是Generator与Discriminator做对抗，一个生成图片，一个判别与真实图片的差别，共同迭代进化。<br><img src="https://img-blog.csdnimg.cn/9e7b6ee3f28443e2a18be74b1f9bb657.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></p><h2 id="三、GAN算法"><a href="#三、GAN算法" class="headerlink" title="三、GAN算法"></a>三、GAN算法</h2><ul><li>Step1:初始化神经网络参数（随机）</li><li>Step2:固定generator，更新discriminator参数<br><img src="https://img-blog.csdnimg.cn/ebf546c20f0d4e8386bbff6e38ee2b8d.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_12,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>训练的目标：分辨真正图片与生成图片之间的差异。能够分得越清楚越好。（可以看做分类器或者回归任务）<br><img src="https://img-blog.csdnimg.cn/d0d3d9955c44411db1907a6bc9182870.png#pic_center" alt="在这里插入图片描述"></li><li>Step3:固定discriminator，训练generator，来达到以假乱真的目的<br><img src="https://img-blog.csdnimg.cn/ea3e21d753c947dcb8f36588a6faf18b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>希望通过训练，使得generator产生的图片，在discriminator的得分尽量。此时，我们可以将generator和discriminator拼做同一个大的神经网络，输入是向量，输出是数值，而且hidden layer里面有一层非常长，对应generator产生的图片向量。</li><li>Step4:反复训练，迭代<h2 id="四、GAN理论"><a href="#四、GAN理论" class="headerlink" title="四、GAN理论"></a>四、GAN理论</h2>训练目标：generator产生的数据的分布，与discriminator的分布越接近越好<br><img src="https://img-blog.csdnimg.cn/fa8b782a261e4d0eb2a8a23ddfe7f61a.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>问题来了，如何衡量这个相似度，或者差异性？<br><img src="https://img-blog.csdnimg.cn/c4060a1c33af48f29eea8705e4cc020b.png#pic_center" alt="在这里插入图片描述"><br>我们可以用JS散度来衡量相似度，定义如下：<br><img src="https://img-blog.csdnimg.cn/2e3213c286a648afbd5db54cb64f7159.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>相当于，我们现在数据集中随机抽样，获得Pdata；再从正态分布中抽样，通过generator得到Pg。这样可以不用知道整个数据集的实际分布。而JS散度的意思是，是时discriminator能够将真实数据分成1类，生成数据分成0类。<br>其实，我们还可以定义其他散度来度量相似度：<br><img src="https://img-blog.csdnimg.cn/fb82316c4c96409581220b7225297814.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><h2 id="五、GAN特点："><a href="#五、GAN特点：" class="headerlink" title="五、GAN特点："></a>五、GAN特点：</h2>（一）不好训练<br>JS Divergence的特点;<br>Pg与Pdata如果没有重叠部分，JS divergence计算出来永远是log2。也就是说，我们看不到整个GAN的改进过程，也很容易在二分类器上达到100%的准确率。<br><img src="https://img-blog.csdnimg.cn/3cd50cb650304c729497b141bca217d6.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"></li></ul><p><strong>改进：</strong> WGAN<br>定义divergence为<br><img src="https://img-blog.csdnimg.cn/aa33e95a0d61457eafef53ecc5ec08ed.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>相当于推土机，从一个分布变成另外一个分布，如何能最短的路径？<br><img src="https://img-blog.csdnimg.cn/dfa836eade394afba3e1df1e3fe9ddae.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>Wasserstein distance就是找到的最短路径，一步一步迭代，而不是直接找到最值。<br><img src="https://img-blog.csdnimg.cn/8565b17752ee4bf7aef0c5a467fafc84.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_13,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="在这里插入图片描述"><br>以上笔记均来自与《李宏毅2021机器学习》，视频链接：<a href="https://www.bilibili.com/video/BV1Wv411h7kN?p=41&amp;share_source=copy_web">https://www.bilibili.com/video/BV1Wv411h7kN?p=41&amp;share_source=copy_web</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MCM&amp;ICM Writing Skills</title>
    <link href="/2022/09/29/%E3%80%90%E6%97%A0%E6%A0%87%E9%A2%98%E3%80%91/"/>
    <url>/2022/09/29/%E3%80%90%E6%97%A0%E6%A0%87%E9%A2%98%E3%80%91/</url>
    
    <content type="html"><![CDATA[<p>标题：基于XX模型或方法做为标题/直接使用赛题所给题目作为标题</p><p>结构：<br><img src="https://img-blog.csdnimg.cn/3d7fc7c2257344a9a445b93c0745beca.png#pic_center" alt="请添加图片描述"><br>写作要点：重要的句子，首次定义的概念，尽量用<strong>黑体</strong>书写。但是不能过多，不然会让论文看起来比较杂乱</p><p><strong>1.</strong>  <strong>Summary</strong></p><p>摘要包含的三要素：<strong>解决了什么问题</strong>；<strong>用了什么方法</strong>；<strong>得到了什么结果</strong></p><p>文字要简练，突出论文的新见解和特色，陈述要客观</p><p>1.1     摘要开头段</p><p>长度控制在3-5行，第一句话：简单交代题目背景（可有可无）；第二句话：交代我们做的事情（必须要有，重要性最高！）；第三句话：解决该问题的实际意义（可有可无）</p><p>1.2     摘要中间段</p><p><strong>解决了什么问题：</strong> 不单独提出，一般与方法写在一起</p><p><strong>应用了什么方法：</strong> 说明求解思路和应用的模型，但一定要紧扣题目本身<br><img src="https://img-blog.csdnimg.cn/1f14274e224441f3bc3b3305f7dfb72c.png#pic_center" alt="请添加图片描述"><br><strong>得到了什么结果：</strong></p><p><strong>1）</strong>   <strong>数值计算答案：</strong> 直接给答案。有重要参数的，可以做灵敏度分析；概率统计的，要给出置信区间；预测类或数值计算，可以给出误差分析。</p><p><strong>2）</strong>   <strong>开放类问题：</strong> 评价类、提建议类：只给出主要结论，有明确支持的观点，不能模棱两可。但如果有数值表述结果更好。比如采用某种方案使结果提升了多少。如果结果太长，可以加一句话引导读者去阅读正文。<br><img src="https://img-blog.csdnimg.cn/ae1a262b86d6437ab0a1d0d311229621.png#pic_center" alt="请添加图片描述"><br>1.3     摘要结尾段</p><p>可以总结一下全文，介绍论文的亮点，也可以对类似的问题进行适当的推广。（但要客观，不能自吹自擂）<br><img src="https://img-blog.csdnimg.cn/3a3f99a8dfd14f02b79409304c4f45cf.png#pic_center" alt="请添加图片描述"><br>1.4     摘要中常见的废话<br><img src="https://img-blog.csdnimg.cn/e300604feab94c908debaa79ff63ee37.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"></p><p><strong>2.</strong>  <strong>Introduction(</strong> <strong>不用全写，最多写三个)</strong></p><p>引言的第一句话，引言的第一段是全文最重要的一个段落。</p><p>用于激发阅读者的阅读兴趣，写的浅显易懂，尽量不用数学表达式。在写作过程中，要反复推敲。</p><p>同时，要用自己的语言重述赛题，明确目标，并对题目出现的模糊概念进行解读。</p><p><strong>2.1</strong>    <strong>Problem Background</strong></p><p>尽量往自己研究的方面去靠。</p><p><strong>2.2</strong>    <strong>Restatement of the Problem</strong></p><p>用自己的话把问题描述一遍（可以和问题背景组合在一起）</p><p><strong>2.3</strong>    <strong>Literature Review</strong></p><p>总结以前的学者针对这个问题已经做的研究。很难写，但是个亮点。</p><p><img src="https://img-blog.csdnimg.cn/79325a653f1540dcbbb5de82d1c705cd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_12,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"></p><p>文献综述是有<strong>目的</strong>的，要和自己的模型或方法相呼应！</p><p><strong>2.4</strong>    <strong>Our work：</strong> 介绍论文的分析思路和建模的框架，有点像问题分析。可以做一个漂亮的图来介绍文章的思路。<br><img src="https://img-blog.csdnimg.cn/e0f5261836154ca2bc0a685004ca25da.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_16,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"><br><img src="https://img-blog.csdnimg.cn/f1afa9d3794b41b0b3fd63afff1cdf6b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_15,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"></p><p><strong>3.</strong>  <strong>Assumptions and Justifications(</strong> <strong>模型假设及合理性说明)</strong></p><p>不要有未解释的假设。<br><img src="https://img-blog.csdnimg.cn/5fc488d63468482d9b49f7f1bb5fa4e0.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_12,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"><img src="https://img-blog.csdnimg.cn/03cd72db96874dabbaf55cf193a1d177.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_7,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"></p><p><strong>4.</strong>  <strong>Notations and Glossary</strong> <strong>(术语汇编)</strong></p><p><img src="https://img-blog.csdnimg.cn/4c6586908dca40e5a6d13860996cdd31.png#pic_center" alt="请添加图片描述"></p><p><img src="https://img-blog.csdnimg.cn/ab9b6d885ea3484792e8e6cf7bc9f83c.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"></p><p><strong>Glossary:</strong> 对专有名词或者模糊的概念进行定义</p><p><strong>5.</strong>  <strong>Model and Solutions</strong> <strong>（每个模型要拆开）</strong></p><p><strong>模型命名：根据建立的模型命名，结合要解决的实际问题</strong><br><img src="https://img-blog.csdnimg.cn/2c10da058d6e4171aca9fb843a8a2978.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"></p><p>每一个模型都分为三部分写： </p><p>\1. 这个模型是什么 </p><p>\2. 这个模型干了什么 </p><p>\3. 这个模型实现了什么样的效果</p><p>描述算法用latex亮点</p><p><strong>6.</strong>  <strong>Data Description</strong> <strong>（不常见）</strong></p><p><strong>6.1</strong>    <strong>Data Collection</strong></p><p><strong>6.2</strong>    <strong>Data Pre-processing/Data Cleaning</strong></p><p><strong>6.3</strong>    <strong>Data Visualization</strong></p><p><strong>6.4</strong>    <strong>Descriptive Statistical Analysis of the Data</strong></p><p>该部分比较灵活，可以放在模型建立与求解，引言，模型准备或者单独一大部分</p><p><strong>7.</strong>  <strong>Sensitivity Analysis</strong> <strong>（非常必要！）</strong></p><p><strong>7.1</strong>    <strong>灵敏度分析</strong></p><p><strong>7.2</strong>    <strong>误差分析</strong></p><p><strong>7.3</strong>    <strong>稳定性检验</strong></p><p><strong>8.</strong>  <strong>Model Evaluation and Future Discussion</strong></p><p><strong>8.1</strong>    <strong>Strengths</strong></p><p><strong>8.2</strong>    <strong>Weaknesses</strong></p><p><strong>8.3</strong>    <strong>Further Discussion:</strong> 模型改进、模型拓展</p><p><strong>9.</strong>  <strong>Conclusion</strong></p><p>论文中心思想的<strong>重申</strong>、<strong>研究结果</strong>或<strong>主要观点</strong>的归纳，或者某些起始性的解释、考虑（与further discussion可能重合）<br><img src="https://img-blog.csdnimg.cn/b7ac7f2acfd64237803480ab1ca879fd.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_11,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"><br>Summary比Our work更<strong>详细</strong>，而且our work<strong>没有结果</strong>。Conclusion跟our work差不多，但有结果。</p><p><strong>10.</strong>      <strong>References</strong></p><p>不要出现中文！自己翻译成英文</p><p><strong>11.</strong>      <strong>Article:</strong></p><p>不要写太专业的话，确保读者能读懂</p><p><strong>《正确书写美国大学生建模竞赛论文》！建议阅读</strong></p><p><strong>美赛论文写作技巧：</strong></p><p><strong>1.</strong>  使用简单时态：一般现在时为主，即便是自己做的工作</p><p><strong>2.</strong>  避免长短句，将从句拆开</p><p><strong>3.</strong>  叙述尽量客观，避免滥用we（在涉及自己主观想法的句子时用we）</p><p><strong>Eg</strong>: We believe, We can infer, We conclude, We recommend, We postulate</p><p><strong>4.</strong>  叙述时突出重点，将重点放在<strong>前面</strong></p><p><strong>Eg:</strong> </p><ul><li><p>差： Apparently, we need to find the shortest tensor route to visit the targets, if we want to save the energy of sensors and achieve the sweep coverage with the minimum number of sensors. </p></li><li><p>好： We can preserve the energy of sensors and achieve sweep coverage using minimum number of sensors by finding the shortest tensor route that visits all targets.</p></li></ul><p><strong>5.</strong>  <strong>避免单调重复……</strong></p><p>Latex使用技巧：</p><p>\1.   <a href="https://www.latexlive.com/home">在线LaTeX公式编辑器-编辑器 (latexlive.com)</a></p><p>\2.   <a href="https://www.tablesgenerator.com/latex_tables">Create LaTeX tables online – TablesGenerator.com</a></p><p>\3.   Overleaf</p><p>\4.   Grammarly语法检查</p><p><strong>美赛论文写作规范：</strong></p><p>我们的目的是引导读者顺利地阅读完整篇论文。比如，需要分析语法才能明白语义，或者回忆前文提到但很久没用引用过的概念，会打断评委的思路。</p><p><strong>1.</strong>  <strong>使用第一人称复数代词</strong></p><p>这样可以增加亲和力。避免使用第二人称或者第三人称</p><p><strong>2.</strong>  <strong>使用简单时态</strong></p><p>想象阅读论文就是随着鼠标的光标开始移动。那么，在光标所在位置应该用一般现在时，描述光标之前的事情用一般过去时，描述光标以后的事情用一般将来时。</p><p>在描述他人的工作时，一般用过去时。但如果是定理或者事实，一般用现在时。</p><p><strong>3.</strong>  <strong>使用主动语态</strong></p><p>主动语态比被动语态更有力、更生动，表达意思更清楚。<br><img src="https://img-blog.csdnimg.cn/9357267c3e1444a5ab78f58f3ebdcf5b.png?x-oss-process=image/watermark,type_d3F5LXplbmhlaQ,shadow_50,text_Q1NETiBAd2VpeGluXzQ3MDc3MTgy,size_10,color_FFFFFF,t_70,g_se,x_16#pic_center" alt="请添加图片描述"><br><strong>4.</strong>  <strong>写简单句子</strong></p><p>写短句，少写从句</p><p><strong>5.</strong>  <strong>写简单段落</strong></p><p>段落的结尾会给读者一个自然停顿的机会。因此段落不宜太长，要给读者一个思考整个段落的机会，但也不能太短，这样读者的思路容易被打断，影响思维的连贯性。</p><p>\6.  使用含有具体含义的词汇</p><p>比如expression equation</p><p>\7.  不要包含过于琐碎的细节</p><p>\8.  突出重点：</p><p>使用黑体标注新概念</p><p>\9.  删掉多余词汇：</p><p>Due to the fact that &amp; since</p><p>\10. 使用并列短语强调相似性</p>]]></content>
    
    
    
    <tags>
      
      <tag>MCM</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Jetson Nano使用笔记</title>
    <link href="/2022/09/29/jetsonnano/"/>
    <url>/2022/09/29/jetsonnano/</url>
    
    <content type="html"><![CDATA[<p>最近刚开学嘛，比较闲，刚好导师有个项目可能需要在终端上部署深度学习算法，就想着自己也搞个玩玩呢。看了网上好多文章之后，也综合对比了一下树莓派和Jetson Nano，最后决定入一个Jetson Nano，毕竟也是用CUDA编程，和导师的项目更加贴近嘛。但是不得不吐槽，板子最近的价格真的贵。。供不应求嘛毕竟。<br>首先，我购买的是摄像头进阶套餐，包含：<br>Jetson Nano B01国产开发套件<br>USB数据线<br>USB无线网卡<br>32GB内存卡<br>读卡器<br>亚克力外壳带风扇<br>网线<br>HDMI高清线<br>电源适配器+电源线<br>IMX219摄像头<br>没有配显示屏，键盘和鼠标呢，这个我有自己的考虑。显示器的话，学校有大屏呢，还有键盘和鼠标，都有现成的，没必要再购买了。而且第一次开机之后，就可以用SSH进行远程连接到笔记本电脑上了，也不用配其他东西了。<br>下面是对Jetson Nano开箱进行理论准备咯：<br>1.1 认识Nano<br>（1）Nano主要接口<br><img src="https://img-blog.csdnimg.cn/5b1ae0723a264d2a96d02deed76fed9c.png" alt="Jetson nano组件图"><br>1-SD卡插槽<br>2-40PIN扩展接头<br>3-USB接口，用于充电或者数据传输<br>4-网线接口<br>5-USB3.0接口<br>6-HDMI接口<br>7-DisplayPort输出端口<br>8-5V电源输入<br>9-CSI摄像机接口<br>1.2 配件准备<br>SD卡与读卡器：建议大小在32G以上，系统烧完卡大概要占13G，还有后续各种框架以及存储深度学习模型文件，因此越大越好<br>USB电源：供电<br>网线：安装以及更新时都必须联网<br>笔记本电脑：烧写SD卡<br>IMX219摄像头：用于计算机视觉应用<br>风扇：在运行深度学习模型时，需要降温<br>1.3 SD卡烧写操作<br>在使用开发套件之前，要对SD卡进行操作，这要可以将Ubuntu操作系统，以及其他工程组件写到SD卡里，方便后面进行开发。其实Jetson Nano就相当于一个小型计算机。<br>那什么是“烧写”呢？烧写也可以说是烧录，就是一次性写入到SD卡里，跟刻录的意义是一样的，但刻录是对应光盘，刻到盘里，烧录是真的“烧”到芯片里。宏观上看不出芯片有什么变化，但是微观上对芯片的结构进行了改变，在上面存储了信息。<br>1.3.1 下载镜像<br>在官网上下载好镜像文件，解压到本地就可以咯<br><a href="https://developer.nvidia.com/embedded/dlc/jetson-nano-dev-kit-sd-card-image">https://developer.nvidia.com/embedded/dlc/jetson-nano-dev-kit-sd-card-image</a><br>1.3.2格式化SD卡<br>使用软件对SD卡进行格式化<br>1.3.3烧录SD卡<br>使用软件写入镜像，进行烧录，大概耗时30Min<br>烧录好之后就可以把SD卡插入到Nano的卡槽里啦</p>]]></content>
    
    
    
    <tags>
      
      <tag>Jetson nano</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/09/28/hello-world/"/>
    <url>/2022/09/28/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
